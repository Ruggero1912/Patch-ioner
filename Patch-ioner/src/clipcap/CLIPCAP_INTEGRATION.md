# ClipCap Integration with Patchioner Class

This document describes how ClipCap models have been integrated into the Patchioner class for DINO feature-based image captioning.

## Overview

ClipCap support has been added to the Patchioner class following the same pattern as other captioning models (VieCap, MeaCap, etc.). This integration allows you to use trained ClipCap models with DINO features for image captioning tasks.

## Architecture

### Files Added/Modified

1. **`src/clipcap/entrypoint.py`** - Main ClipCap integration module
   - `ClipCapModel` class for DINO feature-based captioning
   - Model classes: `ClipCaptionModel`, `ClipCaptionPrefix`, `MLP`, `TransformerMapper`
   - Text generation utilities

2. **`src/model.py`** - Modified Patchioner class
   - Added `clipcap_config` parameter to constructor
   - Added ClipCap initialization logic
   - Added ClipCap support to `caption_tokens` method

3. **Configuration Files**
   - `configs/clipcap_dino_vitb14.k.yaml` - DINOv2-B/14 configuration
   - `configs/clipcap_dino_vitl14.k.yaml` - DINOv2-L/14 configuration

## Configuration

### YAML Configuration Format

```yaml
decap_weights: '/path/to/decap/weights.pt'
prefix_size: 768  # DINO feature dimension
support_memory_size: 0
dino_model: 'dinov2_vitb14'
normalize: True
resize_dim: 518
crop_dim: 518
use_talk2dino_project: False

# ClipCap configuration
clipcap:
  language_model: 'gpt2'
  prefix_length: 10            # Sequence length for prefix
  clip_length: 10              # CLIP sequence length (for transformer mapping)
  num_layers: 8                # Number of transformer layers (for transformer mapping)
  mapping_type: 'mlp'          # 'mlp' or 'transformer'
  only_prefix: True            # Train only prefix mapping vs full model
  temperature: 1.0             # Sampling temperature
  top_p: 0.8                   # Nucleus sampling parameter
  entry_length: 67             # Maximum caption length
  stop_token: '.'              # Stop token for generation
  weight_path: '/path/to/trained/clipcap/model.pt'
```

### Supported DINO Models

The integration automatically detects DINO feature dimensions:

- **DINOv2-S/14**: 384 dimensions (`dinov2_vits14`)
- **DINOv2-B/14**: 768 dimensions (`dinov2_vitb14`)
- **DINOv2-L/14**: 1024 dimensions (`dinov2_vitl14`)
- **DINOv2-G/14**: 1536 dimensions (`dinov2_vitg14`)

## Usage

### 1. Training ClipCap Models

First, train your ClipCap model with DINO features:

```bash
# Extract DINO features
python clipcap_dino_parse_coco.py --dino_model_type dinov2_vitb14

# Train ClipCap model
python clipcapTraining.py \
    --use_dino \
    --dino_model_type dinov2_vitb14 \
    --prefix_length 10 \
    --mapping_type mlp \
    --only_prefix \
    --epochs 10
```

### 2. Using ClipCap with Patchioner

```python
import torch
from src.model import Patchioner

# Load model with ClipCap configuration
device = torch.device('cuda')
model = Patchioner.from_config('configs/clipcap_dino_vitb14.k.yaml', device)

# Generate captions from images
imgs = torch.randn(2, 3, 518, 518).to(device)  # Example batch
results = model.forward(imgs, get_cls_capt=True)
captions = results['cls_capt']

print("Generated captions:")
for i, caption in enumerate(captions):
    print(f"Image {i+1}: {caption}")
```

### 3. Using ClipCap Directly

```python
from src.clipcap.entrypoint import ClipCapModel
import torch

# Configuration
config = {
    'language_model': 'gpt2',
    'prefix_length': 10,
    'mapping_type': 'mlp',
    'only_prefix': True,
    'weight_path': '/path/to/trained/model.pt'
}

# Initialize model
device = torch.device('cuda')
clipcap = ClipCapModel(config, device, dino_feature_dim=768)

# Generate captions from DINO features
dino_features = torch.randn(2, 768).to(device)
captions = clipcap.forward(dino_features)

print(captions)
```

## Performance Improvements

### Batched Text Generation

The ClipCap integration includes an efficient batched text generation implementation:

- **`generate_batched()`**: Processes entire batches simultaneously
- **Significant speedup**: 2-8x faster than sequential processing
- **Memory efficient**: Optimized for GPU memory usage
- **Configurable**: Can fallback to sequential mode if needed

### Configuration Options

```yaml
clipcap:
  use_batched_generation: True  # Enable batched generation (recommended)
  temperature: 1.0              # Sampling temperature
  top_p: 0.8                   # Nucleus sampling parameter
  entry_length: 67             # Maximum sequence length
```

## Model Architecture Details

### ClipCap Model Structure

1. **Input**: DINO features (384/768/1024/1536 dimensions)
2. **Mapping Layer**: 
   - **MLP**: `DINO_dim â†’ GPT2_dim * prefix_length`
   - **Transformer**: Multi-layer transformer mapping
3. **GPT-2 Decoder**: Pretrained GPT-2 for text generation
4. **Output**: Natural language captions

### Key Components

- **`ClipCapModel`**: Main class for DINO-to-text captioning
- **`MLP`/`TransformerMapper`**: Feature mapping from DINO to GPT-2 space
- **Text Generation**: Nucleus sampling with configurable parameters

## Integration with Existing Pipeline

The ClipCap integration follows the established pattern:

1. **Configuration**: YAML-based configuration like other models
2. **Initialization**: Automatic DINO dimension detection
3. **Forward Pass**: Seamless integration with existing forward methods
4. **Scoring**: Optional confidence scoring support

## Testing

Run the integration test:

```bash
python test_clipcap_integration.py
```

This test verifies:
- Configuration loading from YAML
- Model instantiation with ClipCap
- Caption generation with dummy DINO features
- Score computation functionality


## Troubleshooting

### Common Issues

1. **Dimension Mismatch**: Ensure `prefix_size` matches DINO model dimension
2. **Missing Weights**: Verify `weight_path` points to trained ClipCap model
3. **Memory Issues**: Use `only_prefix=True` for lower memory usage
4. **Generation Quality**: Tune `temperature`, `top_p`, and `entry_length`

## References

- [ClipCap Paper](https://arxiv.org/abs/2111.09734)
- [DINO Paper](https://arxiv.org/abs/2104.14294)
- [DINOv2 Paper](https://arxiv.org/abs/2304.07193)